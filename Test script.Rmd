---
title: "D3"
author: "Al√≠cia Chimeno Sarabia"
date: "2023-10-18"
output: html_document
---
```{r Libraries, include=FALSE}
library(dplyr)
library(tidyverse)
library(skimr)
library(VIM)
library(mice)
library(cluster)
require(StatMatch)
```

# Context
# Read data
```{r}
data <- read.csv("reduced_data_sample.csv")
```
# Data Exploratory
# Preprocessing
### Rename the features to a understandable name
```{r}
names(data)
names(data) <- c("id", "target", "contract", "gender", "car", "n_child",
                 "income", "credit", "loan", "price", "job_stat", "studies",
                 "family", "house", "age", "job_duration", "occupation",
                 "job_type", "n_enquiries", "companion")
names(data)
```

### Assign proper type
```{r}
# convert  numerical-> as factor
data$target <- factor(data$target, levels = c(0,1), labels = c("payed","overdue"))
# convert character -> as factor:
char_cols <- which(sapply(data, is.character))
data[, char_cols] <- lapply(data[, char_cols], as.factor) 
str(data)
summary(data)
```

### Transform variables 
First, we have an age variable (age when they applied to the loan) that is a negative number and computed as the days spent in their lifetime, we want to save the age they had when they applied to the loan in years. We created the new variable transformed_age. 
```{r}
data$age<-floor(abs(data$age) / 365)
```

Second, we transform the job_duration variable (#days they worked when they applied to the loan). Also calculated with days. We think it is more understandable to see the years. 

When we have a glimpse of the values, we can see an error in the data set. The value 365243 clearly is a wrong value because it is equivalent to 1000 years worked and it is found in many records. We will transform it to a missing value. 
We also rewrite the missing values of the factor variables as "Unknown". 
```{r}
# data$job_duration
data$job_duration[which(data$job_duration == 365243)] <- NA
data$job_duration <- (abs(data$job_duration) / 365) # job duration in years
data$job_type <- as.character(data$job_type)
data$job_type[which(data$job_type == "XNA")] <- "Unknown"
data$job_type <- as.factor(data$job_type)

data$gender <- as.character(data$gender)
data$gender[which(data$gender == "XNA")] <- "Unknown"
data$gender <- as.factor(data$gender)

data$occupation <- as.character(data$occupation)
data$occupation[which(data$occupation == "")] <- "Unknown"
data$occupation <- as.factor(data$occupation)

data$companion <- as.character(data$companion)
data$companion[which(data$companion == "")] <- "Unknown"
data$companion <- as.factor(data$companion)

```

### Check if the values and NA's are correctly saved
We assign to the NA's in factor variables as "Unknown". 
```{r}
summary(data)
```
The variables that have (other) : occupation, job_type,companion: means that there are more classes. Let's explore them: 

```{r}
names(table(data$occupation))
# What does the following code do?
# data <- data %>%
#  mutate_all(~replace(., . == "XNA", "Unknown"))
# print(unique(data$occupation))
names(table(data$job_type))
names(table(data$companion))
```
# NA's + Unknowns ratio
We calculated the percentage of missing values
```{r}
missing.values.data <-  as.data.frame(skimr::skim(data))
na_perc = sum(missing.values.data$n_missing) / (ncol(data) * nrow(data)) * 100
unknown_count = sum(data$job_type == "Unknown", na.rm = T) + sum(data$gender == "Unknown") +
  sum(data$occupation == "Unknown") + sum(data$companion == "Unknown")
unknown_perc = unknown_count / (ncol(data) * nrow(data)) * 100
missing_perc = na_perc + unknown_perc
```

Attention, there is only one numerical variable with missing values: 'n_enquiries'.
Percentage of missing values:
```{r}
689/nrow(data)*100
```

We were not able to install the BaylorEdPsych package, so we copied the function here:
```{r, include=FALSE}
LittleMCAR<-function(x){
 if (!require(mvnmle)) 
	stop("You must have mvnmle installed to use LittleMCAR")
 if (!(is.matrix(x) | is.data.frame(x))) 
        stop("Data should be a matrix or dataframe")
if (is.data.frame(x)) 
        x <- data.matrix(x)
# define variables        
n.var<-ncol(x) # number of variables
n<-nrow(x)  #number of respondents
var.names<-colnames(x)
r <- 1 * is.na(x)
nmis <- as.integer(apply(r, 2, sum))  #number of missing data for each variable
mdp <- (r %*% (2^((1:n.var - 1)))) + 1  #missing data patterns
x.mp<-data.frame(cbind(x,mdp))
colnames(x.mp)<-c(var.names,"MisPat")
n.mis.pat<-length(unique(x.mp$MisPat)) # number of missing data patterns
p<-n.mis.pat-1
gmean<-mlest(x)$muhat #ML estimate of grand mean (assumes Normal dist)
gcov<-mlest(x)$sigmahat #ML estimate of grand covariance (assumes Normal dist)
colnames(gcov)<-rownames(gcov)<-colnames(x)

#recode MisPat variable to go from 1 through n.mis.pat
x.mp$MisPat2<-rep(NA,n)
for (i in 1:n.mis.pat){
x.mp$MisPat2[x.mp$MisPat == sort(unique(x.mp$MisPat), partial=(i))[i]]<- i
}
x.mp$MisPat<-x.mp$MisPat2
x.mp<-x.mp[ , -which(names(x.mp) %in% "MisPat2")]

#make list of datasets for each pattern of missing data
datasets<-list() 
for (i in 1:n.mis.pat){
datasets[[paste("DataSet",i,sep="")]]<-x.mp[which(x.mp$MisPat==i),1:n.var]
}

#degrees of freedom
kj<-0
for (i in 1:n.mis.pat){	
no.na<-as.matrix(1* !is.na(colSums(datasets[[i]])))
kj<-kj+colSums(no.na)
}
df<-kj -n.var

#Little's chi-square
d2<-0
cat("this could take a while")
for (i in 1:n.mis.pat){	
mean<-(colMeans(datasets[[i]])-gmean)
mean<-mean[!is.na(mean)]
keep<-1* !is.na(colSums(datasets[[i]]))
keep<-keep[which(keep[1:n.var]!=0)]
cov<-gcov 
cov<-cov[which(rownames(cov) %in% names(keep)) , which(colnames(cov) %in% names(keep))]
d2<-as.numeric(d2+(sum(x.mp$MisPat==i)*(t(mean)%*%solve(cov)%*%mean)))
}

#p-value for chi-square
p.value<-1-pchisq(d2,df)

#descriptives of missing data
amount.missing<-matrix(nmis, 1, length(nmis))
percent.missing<-amount.missing/n
amount.missing<-rbind(amount.missing,percent.missing)
colnames(amount.missing)<-var.names
rownames(amount.missing)<-c("Number Missing", "Percent Missing")

list(chi.square=d2, df=df, p.value=p.value, missing.patterns=n.mis.pat, amount.missing=amount.missing, data=datasets)

}
```

```{r}
little.test <- LittleMCAR(data)
```
```{r}
little.test$chi.square
```
=> data is NOT missing completely at random
```{r}
little.test$p.value
```
```{r}
little.test$df
```
```{r}
little.test$missing.patterns
```
```{r}
little.test$amount.missing
```
```{r}
# select numerical variables
numerical_data <- data %>%
  select_if(is.numeric)

md.pattern(numerical_data)
```

```{r}
# Look the NA's with VIM packages
mice_plot <- aggr(numerical_data, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(numerical_data), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
```
```{r}
# use mice to impute the missing numerical values
imputed_Data <- mice(numerical_data, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imputed_Data)
```
```{r}
# inspect quality of imputations
stripplot(imputed_Data, price, pch = 19, xlab = "Imputation number")
imputed_Data$imp$price

# get complete data ( 3nd out of 5)
completeData <- mice::complete(imputed_Data, 3)
```
```{r}
stripplot(imputed_Data, job_duration, pch = 19, xlab = "Imputation number")

```
```{r}
stripplot(imputed_Data, n_enquiries, pch = 19)

```


```{r}
stripplot(imputed_Data, pch = 10, cex = 1.2)
```
```{r}
densityplot(imputed_Data)
```


knn for numerical imputations
```{r}
knn_imputed <- kNN(numerical_data)
```

```{r}
marginplot(knn_imputed[, c("price", "job_duration")])
```
MIMI function for numerical imputations
```{r, include=FALSE}
#assume missings represented with NA
uncompleteVar<-function(vector){any(is.na(vector))}

Mode <- function(x) 
{
  x<-as.factor(x)
  maxV<-which.max(table(x))
  return(levels(x)[maxV])
}



MiMMi <- function(data, priork=-1)
{
  #Identify columns without missings
  colsMiss<-which(sapply(data, uncompleteVar))
  if(length(colsMiss)==0){
    print("Non missing values found")
    out<-dd
    }else{
    K<-dim(data)[2]
    colsNoMiss<-setdiff(c(1:K),as.vector(colsMiss))
  
    #cluster with complete data
    dissimMatrix <- daisy(data[,colsNoMiss], metric = "gower", stand=TRUE)
    distMatrix<-dissimMatrix^2
  
    hcdata<-hclust(distMatrix, method = "ward.D2")
    plot(hcdata)
    nk<-2
    if(priork==-1){
      print("WARNING: See the dendrogramm and ZOOM if required")
      print("and enter a high number of clusters")
      nk<-readline("(must be a positive integer). k: ")
      nk<-as.integer(nk)
    }else{nk<-priork}
  
    partition<-cutree(hcdata, nk)

    CompleteData<-data
    #nomes cal per tenir tra?a de com s'ha fet la substituci?
    newCol<-K+1
    CompleteData[,newCol]<-partition
    names(CompleteData)[newCol]<-"ClassAux"
  
    setOfClasses<-as.numeric(levels(as.factor(partition)))
    imputationTable<-data.frame(row.names=setOfClasses)
    p<-1
  
    for(k in colsMiss)
    {
       #Files amb valors utils
       rowsWithFullValues<-!is.na(CompleteData[,k])
    
       #calcular valors d'imputacio
       if(is.numeric(CompleteData[,k]))
       {
          imputingValues<-aggregate(CompleteData[rowsWithFullValues,k], by=list(partition[rowsWithFullValues]), FUN=mean)
       }else{
          imputingValues<-aggregate(CompleteData[rowsWithFullValues,k], by=list(partition[rowsWithFullValues]), FUN=Mode)
       }
    
       #Impute
    
       for(c in setOfClasses)
       {
          CompleteData[is.na(CompleteData[,k]) & partition==c,k]<-imputingValues[c,2]
       }
    
       #Imputation Table
       imputationTable[,p]<-imputingValues[,2]
       names(imputationTable)[p]<-names(data)[k]
       p<-p+1
    }
    
    rownames(imputationTable)<-paste0("c", 1:nk)
    out<-new.env()
    out$imputedData<-CompleteData
    out$imputation<-imputationTable
  }
  return(out)
}
```
```{r}
mimi_imputed<-MiMMi(numerical_data)
```
```{r}
mimi_imputed$imputation
```
```{r}
mimi_imputed$imputedData
```

```{r}
densityplot(data$price, col = "blue", main = "Density Plot of Price")
densityplot(mimi_imputed$imputedData$price, col = "blue", add = TRUE)
```
```{r}
densityplot(mimi_imputed$imputedData$job_duration, col = "blue", add = TRUE)
densityplot(data$job_duration, col = "blue", add = TRUE)


```
```{r}
densityplot(mimi_imputed$imputedData$n_enquiries, col = "blue", add = TRUE)
densityplot(data$n_enquiries, col = "blue", add = TRUE)
```
```{r}
densityplot(completeData$job_duration, col = "blue", add = TRUE)
densityplot(data$job_duration, col = "blue", add = TRUE)
```
```{r}
densityplot(completeData$n_enquiries, col = "blue", add = TRUE)
densityplot(data$n_enquiries, col = "blue", add = TRUE)
```
```{r}
densityplot(completeData$price, col = "blue", add = TRUE)
densityplot(data$price, col = "blue", add = TRUE)
```
We decided to use the numerical imputations from mice
```{r}
columns_to_replace <- c("price", "n_enquiries", "job_duration")
data[columns_to_replace] <- completeData[columns_to_replace]
head(data)
```

Now we did a PCA only for numerical variables to see how variables are related

```{r}
numeriques<-which(sapply(data,is.numeric))
numeriques

dcon<-data[,numeriques]
sapply(dcon,class)

#dcon <- data.frame (Antiguedad.Trabajo,Plazo,Edad,Gastos,Ingresos,Patrimonio,Cargas.patrimoniales,Importe.solicitado,Precio.del.bien.financiado,Estalvi, RatiFin)

#alternatively
#dim(dd)
#indexCon<-c(2,4:5,9:16)
#dcon<-dd[,indexCon]
#names(dcon)

#be sure you don't have missing data in your numerical variables.

#in case of having missing data, select complete rows JUST TO FOLLOW THE CLASS
#dd<-dd[!is.na(dd[,indecCon[1]])& !is.na(dd[,indecCon[2]]) & !is.na(dd[,indecCon[3]])& !is.na(dd[,indecCon[4]]),]
#then preprocess your complete data set to IMPUTE all missing data, and reproduce
#the whole analysis again
# PRINCIPAL COMPONENT ANALYSIS OF dcon

pc1 <- prcomp(dcon, scale=TRUE)
class(pc1)
attributes(pc1)

print(pc1)

str(pc1)


# WHICH PERCENTAGE OF THE TOTAL INERTIA IS REPRESENTED IN SUBSPACES?

pc1$sdev
inerProj<- pc1$sdev^2 
inerProj
totalIner<- sum(inerProj)
totalIner
pinerEix<- 100*inerProj/totalIner
pinerEix
barplot(pinerEix)

#Cummulated Inertia in subspaces, from first principal component to the 11th dimension subspace
barplot(100*cumsum(pc1$sdev[1:dim(dcon)[2]]^2)/dim(dcon)[2])
percInerAccum<-100*cumsum(pc1$sdev[1:dim(dcon)[2]]^2)/dim(dcon)[2]
percInerAccum


# SELECTION OF THE SINGIFICNT DIMENSIONS (keep 80% of total inertia)

nd = 5

print(pc1)
attributes(pc1)
pc1$rotation

# STORAGE OF THE EIGENVALUES, EIGENVECTORS AND PROJECTIONS IN THE nd DIMENSIONS
View(pc1$x)
dim(pc1$x)
dim(dcon)
dcon[2000,]
pc1$x[2000,]

Psi = pc1$x[,1:nd]
dim(Psi)
Psi[2000,]

# STORAGE OF LABELS FOR INDIVIDUALS AND VARIABLES

iden = row.names(dcon)
etiq = names(dcon)
ze = rep(0,length(etiq)) # WE WILL NEED THIS VECTOR AFTERWARDS FOR THE GRAPHICS

# PLOT OF INDIVIDUALS

#select your axis
#eje1<-2
eje1<-1
#eje2<-3
eje2<-2

# Here is were we create the shadows

plot(Psi[,eje1],Psi[,eje2])
text(Psi[,eje1],Psi[,eje2],labels=iden, cex=0.5)
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

# Now don't display the nodes

plot(Psi[,eje1],Psi[,eje2], type="n")
text(Psi[,eje1],Psi[,eje2],labels=iden, cex=0.5)
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")
#library(rgl)
#plot3d(Psi[,1],Psi[,2],Psi[,3])

#Projection of variables

# Correlation between original variables and the principal components
Phi = cor(dcon,Psi)
View(Phi)

#select your axis

X<-Phi[,eje1]
Y<-Phi[,eje2]

plot(Psi[,eje1],Psi[,eje2],type="n")
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)
```

```{r}
pinerEix<- 100*inerProj/totalIner
pinerEix
barplot(pinerEix)

#Cummulated Inertia in subspaces, from first principal component to the 11th dimension subspace
barplot(100*cumsum(pc1$sdev[1:dim(dcon)[2]]^2)/dim(dcon)[2])
percInerAccum<-100*cumsum(pc1$sdev[1:dim(dcon)[2]]^2)/dim(dcon)[2]
percInerAccum
```
```{r}
plot(Psi[,eje1],Psi[,eje2], type="n")
#text(Psi[,eje1],Psi[,eje2],labels=iden, cex=0.5)
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")
```


```{r}
plot(Psi[,eje1],Psi[,eje2],type="n", xlim = c(-1,1), ylim = c(-1, 1))
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)
```

```{r}
# Zoom to see the aggregated variables among the negative first component

plot(Psi[,eje1],Psi[,eje2],type="n", xlim = c(-1,-0.7), ylim = c(-0.2, 0.2))
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)
```

```{r}
# Check that there are not more variables aggregated close to "loan"

plot(Psi[,eje1],Psi[,eje2],type="n", xlim = c(-1,-0.85), ylim = c(-0.1, 0))
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)
```

 Afterwards, categorical variables are projected into PCA to have a better 
 understanding of the factorial plains

```{r}
# Adding the target variable (index=2)
varcat=factor(data[,2])
plot(Psi[,1],Psi[,2], pch=c(1,20) [varcat], col=c("lightgreen", "red") [varcat])
axis(side=1, pos= 0, labels = F, col="darkgray")
axis(side=3, pos= 0, labels = F, col="darkgray")
axis(side=2, pos= 0, labels = F, col="darkgray")
axis(side=4, pos= 0, labels = F, col="darkgray")
legend("bottomleft",levels(factor(varcat)),pch=c(1,20),col=c(1,2), cex=0.6)

```
```{r}
# From the previous plot, check where the centroids of "paid" and "overdue" are
varcat<-factor(data[,2]) 
fdic1 = tapply(Psi[,eje1],varcat,mean)
fdic2 = tapply(Psi[,eje2],varcat,mean) 

plot(Psi[,eje1],Psi[,eje2],type="n")
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

text(fdic1,fdic2,labels=levels(varcat), col=c("green", "red"), 
     cex=0.7)
```

```{r}
dcat <- c(2:5,11:14,17:18,20)
colors<-rainbow(length(dcat))

plot(Psi[,eje1],Psi[,eje2], type = "n")
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

# Loop to add all categories 

c <- 1
for(k in dcat){
  seguentColor<-colors[c]
  fdic1 = tapply(Psi[,eje1],data[,k],mean)
  fdic2 = tapply(Psi[,eje2],data[,k],mean) 
  
  text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, cex=0.6)
  c<-c+1
}
legend("bottomleft",names(data)[dcat],pch=1,col=colors, cex=0.6)

```

```{r}
# A zoom is required 
dcat <- c(2:5,11:14,17:18,20)
colors<-rainbow(length(dcat))

plot(Psi[,eje1],Psi[,eje2], type = "n", xlim = c(-3,2), ylim = c(-2,3))
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

# Loop to add all categories 

c <- 1
for(k in dcat){
  seguentColor<-colors[c]
  fdic1 = tapply(Psi[,eje1],data[,k],mean)
  fdic2 = tapply(Psi[,eje2],data[,k],mean) 
  
  text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, cex=0.6)
  c<-c+1
}
legend("bottomleft",names(data)[dcat],pch=1,col=colors, cex=0.6)

```
```{r}
# As there are too much variables its important to do several maps that will 
# share the same length of the axis to be comparable visually
# Note: There is a bug in the visualization. The last plot seams smaller
# but when you save the figure all of them have the same size

groups <- list(c(2:5), c(11:13), 14, 17, 18, 20)

for(j in 1:length(groups)){
  colors <- rainbow(length(groups))
  
  a = plot(Psi[,eje1],Psi[,eje2], type = "n", xlim = c(-3,2), ylim = c(-2,3))
  axis(side=1, pos= 0, labels = F, col="cyan")
  axis(side=3, pos= 0, labels = F, col="cyan")
  axis(side=2, pos= 0, labels = F, col="cyan")
  axis(side=4, pos= 0, labels = F, col="cyan")
    
    # Loop to add all categories of this group
    c <- 1
    for(k in groups[[j]]){
      seguentColor<-colors[c]
      fdic1 = tapply(Psi[,eje1],data[,k],mean)
      fdic2 = tapply(Psi[,eje2],data[,k],mean) 
      
      text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, 
           cex=0.6)
      c <- c+1
    }
  legend("bottomleft",names(data)[groups[[j]]],pch=1,col=colors, cex=0.6)
}


```
```{r}
# Numerical variables and centroids of desired categorical variables
# Several maps are created to see relations

groups <- list(c(2:5), c(11:13))

for(j in 1:length(groups)){
  colors <- rainbow(length(groups))
  
  a = plot(Psi[,eje1],Psi[,eje2], type = "n", xlim = c(-1,2), ylim = c(-1,3))
  axis(side=1, pos= 0, labels = F, col="cyan")
  axis(side=3, pos= 0, labels = F, col="cyan")
  axis(side=2, pos= 0, labels = F, col="cyan")
  axis(side=4, pos= 0, labels = F, col="cyan")
  
  #add projections of numerical variables in background
  arrows(ze, ze, X, Y, length = 0.07,col="lightgray")
  text(X,Y,labels=etiq,col="gray", cex=0.7)
    
    # Loop to add all categories of this group
    c <- 1
    for(k in groups[[j]]){
      seguentColor<-colors[c]
      fdic1 = tapply(Psi[,eje1],data[,k],mean)
      fdic2 = tapply(Psi[,eje2],data[,k],mean) 
      
      text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, 
           cex=0.6)
      c <- c+1
    }
  legend("bottomleft",names(data)[groups[[j]]],pch=1,col=colors, cex=0.6)
}

```


# Outlier detection 
use Mahalanobis distance with principal component analysis



