---
title: "D3"
author: "Al√≠cia Chimeno Sarabia"
date: "2023-10-18"
output: html_document
---
# Library importation
```{r Libraries, include=FALSE}
library(dplyr)
library(tidyverse)
library(skimr)
library(VIM)
library(mice)
library(cluster)
library(ggplot2)
library(car)
require(StatMatch)
```


##### Data Overview #####

# Read data
```{r}
data <- read.csv("Bank-India-data.csv")

summary(data)
```


##### Preprocessing - Basic Transformation #####

# Rename the features to a naunced name
```{r}
names(data) <- c("id", "target", "contract", "gender", "car", "n_child",
                 "income", "credit", "loan", "price", "job_stat", "studies",
                 "family", "house", "age", "job_duration", "occupation",
                 "job_type", "n_enquiries", "companion")
names(data)
```

# Rename the categorical feature in a more concise name
```{r}
data$job_stat <- fct_recode(data$job_stat, "Commer. Assoc." = "Commercial associate")
data$family <- fct_recode(data$family,  "single" = "Single / not married", "divorce" = "Separated")
data$house <- fct_recode(data$house, "apartment" = "House / apartment", "Municipal apart." = "Municipal apartment", "Rented apart."="Rented apartment", "Office apart."="Office apartment", "Co-op apart."="Co-op apartment")
data$occupation <- fct_recode(data$occupation, "High-tech stf" = "High skill tech staff", "Medic stf" = "Medicine staff", "Chef" = "Cooking staff", "Security" = "Security staff", "Waiters" = "Waiters/barmen staff", "Low-skill labor." = "Low-skill Laborers", "Private ser." = "Private service staff")
data$companion <- fct_recode(data$companion, "Partner" = "Spouse, partner", "Unaccompan." = "Unaccompanied", "Group_people" = "Group of people")
data$studies <- fct_recode(data$studies, "Secondary" = "Secondary / secondary special", "Higher edu." = "Higher education")
```

# Transform the numerical & characters into factor.
```{r}
# convert numerical target (0,1) -> as factor ("payed", "overdue")
data$target <- factor(data$target, levels = c(0,1), labels = c("payed","overdue"))

# convert character feature (such as gender, car or contract type) -> as factor:
char_cols <- which(sapply(data, is.character))
data[, char_cols] <- lapply(data[, char_cols], as.factor)

# Review if the transformation has executed properly
str(data)
summary(data)
```

# Transform variables 
# First, we have an age feature (age when they applied to the loan) that is a negative number and computed as the days spent in their lifetime until the moment they apply for the loan. We want to transform it into years, for a more comprehensive purpose.
```{r}
data$age<-floor(abs(data$age) / 365)
```

# Second, we transform the job_duration variable (#days they worked when they applied to the loan in the current work). Also calculated with days. We think it is more understandable to see the years. 

# When we have a glimpse of the values, we can see an error in this feature. There is a value of 365243 days working, which cleary is a wrong value as it's equivalent to 1000 years worked. This value is recorded when the job_type equal to XNA, which means the applicant does not have any work at the time of application.
# So we also rewrite the missing values of the factor variables as "Unknown". 
```{r}
# data$job_duration
data$job_duration[which(data$job_duration == 365243)] <- NA
data$job_duration <- (abs(data$job_duration) / 365) # job duration in years
data$job_type <- as.character(data$job_type)
data$job_type[which(data$job_type == "XNA")] <- "Unknown"
data$job_type <- as.factor(data$job_type)

data$gender <- as.character(data$gender)
data$gender[which(data$gender == "XNA")] <- "Unknown"
data$gender <- as.factor(data$gender)

data$occupation <- as.character(data$occupation)
data$occupation[which(data$occupation == "")] <- "Unknown"
data$occupation <- as.factor(data$occupation)

data$companion <- as.character(data$companion)
data$companion[which(data$companion == "")] <- "Unknown"
data$companion <- as.factor(data$companion)

```

# Check if the values and NA's are correctly saved
# We assign to the NA's in factor variables as "Unknown". 
```{r}
summary(data)
```

# The variables that have (other) : occupation, job_type,companion: means that there are more classes. Let's explore them: 
```{r}

# What does the following code do?
# data <- data %>%
#  mutate_all(~replace(., . == "XNA", "Unknown"))
# print(unique(data$occupation))
names(table(data$occupation))
names(table(data$job_type))
names(table(data$companion))
```

# We calculated the percentage of missing values which is NA's + Unknowns ratio
```{r}
missing.values.data <-  as.data.frame(skimr::skim(data))
na_perc = sum(missing.values.data$n_missing) / (ncol(data) * nrow(data)) * 100
unknown_count = sum(data$job_type == "Unknown", na.rm = T) + sum(data$gender == "Unknown") +
  sum(data$occupation == "Unknown") + sum(data$companion == "Unknown")
unknown_perc = unknown_count / (ncol(data) * nrow(data)) * 100
missing_perc = na_perc + unknown_perc
```

# Attention, there is only one numerical variable with missing values: 'n_enquiries'. Percentage of missing values:
# What's the purpose of this line?
```{r}
689/nrow(data)*100
```

# As we were not able to install the BaylorEdPsych package, we had to copy the function here:
```{r, include=FALSE}
LittleMCAR<-function(x){
 if (!require(mvnmle)) 
	stop("You must have mvnmle installed to use LittleMCAR")
 if (!(is.matrix(x) | is.data.frame(x))) 
        stop("Data should be a matrix or dataframe")
if (is.data.frame(x)) 
        x <- data.matrix(x)
# define variables        
n.var<-ncol(x) # number of variables
n<-nrow(x)  #number of respondents
var.names<-colnames(x)
r <- 1 * is.na(x)
nmis <- as.integer(apply(r, 2, sum))  #number of missing data for each variable
mdp <- (r %*% (2^((1:n.var - 1)))) + 1  #missing data patterns
x.mp<-data.frame(cbind(x,mdp))
colnames(x.mp)<-c(var.names,"MisPat")
n.mis.pat<-length(unique(x.mp$MisPat)) # number of missing data patterns
p<-n.mis.pat-1
gmean<-mlest(x)$muhat #ML estimate of grand mean (assumes Normal dist)
gcov<-mlest(x)$sigmahat #ML estimate of grand covariance (assumes Normal dist)
colnames(gcov)<-rownames(gcov)<-colnames(x)

#recode MisPat variable to go from 1 through n.mis.pat
x.mp$MisPat2<-rep(NA,n)
for (i in 1:n.mis.pat){
x.mp$MisPat2[x.mp$MisPat == sort(unique(x.mp$MisPat), partial=(i))[i]]<- i
}
x.mp$MisPat<-x.mp$MisPat2
x.mp<-x.mp[ , -which(names(x.mp) %in% "MisPat2")]

#make list of datasets for each pattern of missing data
datasets<-list() 
for (i in 1:n.mis.pat){
datasets[[paste("DataSet",i,sep="")]]<-x.mp[which(x.mp$MisPat==i),1:n.var]
}

#degrees of freedom
kj<-0
for (i in 1:n.mis.pat){	
no.na<-as.matrix(1* !is.na(colSums(datasets[[i]])))
kj<-kj+colSums(no.na)
}
df<-kj -n.var

#Little's chi-square
d2<-0
cat("this could take a while")
for (i in 1:n.mis.pat){	
mean<-(colMeans(datasets[[i]])-gmean)
mean<-mean[!is.na(mean)]
keep<-1* !is.na(colSums(datasets[[i]]))
keep<-keep[which(keep[1:n.var]!=0)]
cov<-gcov 
cov<-cov[which(rownames(cov) %in% names(keep)) , which(colnames(cov) %in% names(keep))]
d2<-as.numeric(d2+(sum(x.mp$MisPat==i)*(t(mean)%*%solve(cov)%*%mean)))
}

#p-value for chi-square
p.value<-1-pchisq(d2,df)

#descriptives of missing data
amount.missing<-matrix(nmis, 1, length(nmis))
percent.missing<-amount.missing/n
amount.missing<-rbind(amount.missing,percent.missing)
colnames(amount.missing)<-var.names
rownames(amount.missing)<-c("Number Missing", "Percent Missing")

list(chi.square=d2, df=df, p.value=p.value, missing.patterns=n.mis.pat, amount.missing=amount.missing, data=datasets)

}
```

# Run the little test to see if the missing value are MCAR, MAR or MNAR type
```{r}
little.test <- LittleMCAR(data)
```
```{r}
little.test$chi.square
```

# Data is NOT missing completely at random as p-value is 0.
```{r}
little.test$p.value
```
```{r}
little.test$df
```
```{r}
little.test$missing.patterns
```
```{r}
little.test$amount.missing
```
```{r}
# select numerical variables
numerical_data <- data %>%
  select_if(is.numeric)

md.pattern(numerical_data)
```


######  Preprocessing - Data Imputation #######

# Use MICE to impute the data
```{r}
# Look the NA's with VIM packages
mice_plot <- aggr(numerical_data, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(numerical_data), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))
```
```{r}
# use mice to impute the missing numerical values
imputed_Data <- mice(numerical_data, m=5, maxit = 50, method = 'pmm', seed = 500)
summary(imputed_Data)
```
```{r}
# inspect quality of imputations
stripplot(imputed_Data, price, pch = 19, xlab = "Imputation number")
imputed_Data$imp$price

# get complete data ( 3nd out of 5)
completeData <- mice::complete(imputed_Data, 3)
```
```{r}
stripplot(imputed_Data, job_duration, pch = 19, xlab = "Imputation number")
```
```{r}
stripplot(imputed_Data, n_enquiries, pch = 19)
```
```{r}
stripplot(imputed_Data, pch = 10, cex = 1.2)
```
```{r}
densityplot(imputed_Data)
```

# KNN for numerical imputations
```{r}
knn_imputed <- kNN(numerical_data)
```
```{r}
marginplot(knn_imputed[, c("price", "job_duration")])
```

# MIMI function for numerical imputations
```{r, include=FALSE}
#assume missings represented with NA
uncompleteVar<-function(vector){any(is.na(vector))}

Mode <- function(x) 
{
  x<-as.factor(x)
  maxV<-which.max(table(x))
  return(levels(x)[maxV])
}

MiMMi <- function(data, priork=-1)
{
  #Identify columns without missings
  colsMiss<-which(sapply(data, uncompleteVar))
  if(length(colsMiss)==0){
    print("Non missing values found")
    out<-dd
    }else{
    K<-dim(data)[2]
    colsNoMiss<-setdiff(c(1:K),as.vector(colsMiss))
  
    #cluster with complete data
    dissimMatrix <- daisy(data[,colsNoMiss], metric = "gower", stand=TRUE)
    distMatrix<-dissimMatrix^2
  
    hcdata<-hclust(distMatrix, method = "ward.D2")
    plot(hcdata)
    nk<-2
    if(priork==-1){
      print("WARNING: See the dendrogramm and ZOOM if required")
      print("and enter a high number of clusters")
      nk<-readline("(must be a positive integer). k: ")
      nk<-as.integer(nk)
    }else{nk<-priork}
  
    partition<-cutree(hcdata, nk)

    CompleteData<-data
    #nomes cal per tenir tra?a de com s'ha fet la substituci?
    newCol<-K+1
    CompleteData[,newCol]<-partition
    names(CompleteData)[newCol]<-"ClassAux"
  
    setOfClasses<-as.numeric(levels(as.factor(partition)))
    imputationTable<-data.frame(row.names=setOfClasses)
    p<-1
  
    for(k in colsMiss)
    {
       #Files amb valors utils
       rowsWithFullValues<-!is.na(CompleteData[,k])
    
       #calcular valors d'imputacio
       if(is.numeric(CompleteData[,k]))
       {
          imputingValues<-aggregate(CompleteData[rowsWithFullValues,k], by=list(partition[rowsWithFullValues]), FUN=mean)
       }else{
          imputingValues<-aggregate(CompleteData[rowsWithFullValues,k], by=list(partition[rowsWithFullValues]), FUN=Mode)
       }
    
       #Impute
    
       for(c in setOfClasses)
       {
          CompleteData[is.na(CompleteData[,k]) & partition==c,k]<-imputingValues[c,2]
       }
    
       #Imputation Table
       imputationTable[,p]<-imputingValues[,2]
       names(imputationTable)[p]<-names(data)[k]
       p<-p+1
    }
    
    rownames(imputationTable)<-paste0("c", 1:nk)
    out<-new.env()
    out$imputedData<-CompleteData
    out$imputation<-imputationTable
  }
  return(out)
}
```
```{r}
mimi_imputed<-MiMMi(numerical_data)
```
```{r}
mimi_imputed$imputation
```
```{r}
mimi_imputed$imputedData
```

# Density Plot
```{r}
densityplot(data$price, col = "blue", main = "Density Plot of Price")
densityplot(mimi_imputed$imputedData$price, col = "blue", add = TRUE)
```
```{r}
densityplot(mimi_imputed$imputedData$job_duration, col = "blue", add = TRUE)
densityplot(data$job_duration, col = "blue", add = TRUE)

```
```{r}
densityplot(mimi_imputed$imputedData$n_enquiries, col = "blue", add = TRUE)
densityplot(data$n_enquiries, col = "blue", add = TRUE)
```
```{r}
densityplot(completeData$job_duration, col = "blue", add = TRUE)
densityplot(data$job_duration, col = "blue", add = TRUE)
```
```{r}
densityplot(completeData$n_enquiries, col = "blue", add = TRUE)
densityplot(data$n_enquiries, col = "blue", add = TRUE)
```
```{r}
densityplot(completeData$price, col = "blue", add = TRUE)
densityplot(data$price, col = "blue", add = TRUE)
```

# Based on the performances of each imputation method that we had for our dataset, We decided to use MICE for numerical imputations
```{r}
columns_to_replace <- c("price", "n_enquiries", "job_duration")
data[columns_to_replace] <- completeData[columns_to_replace]
head(data)
```


##### PCA #####

# Firstly, we did a PCA only for numerical variables to see how the variables are related. The result of the 9 principal components shows that keeping first 5 components are required in order to explain 80% of the accumulated inertia. 
# Using the two main components (50% of accumulated inertia) we can see that how variables like "price", "credit" and "loan" have the most extreme values of PC1 whereas "income" is located at the middle. This suggest that the first axis is related to an economical pattern. While in the second axis, it's related with time, such as age and job duration. 
#~The n_child is seems to be an outlier, but we'll comment it later.

```{r, include=FALSE}
# Here we can see the output of the PCA 
numeriques <- which(sapply(data,is.numeric))
numeriques

dcon <- data[,numeriques]
sapply(dcon,class)

pc1 <- prcomp(dcon, scale=TRUE)
pc1
```

# Accumulated Inertia in subspace, from first principal component to the 11th dimension subspace. Which we can observe that 5 components are required in order to achieve the 80% of accumulated inertia.
```{r}
barplot(100*cumsum(pc1$sdev[1:dim(dcon)[2]]^2)/dim(dcon)[2], main = 
        'Analysis of principal components', 
        names.arg = c("PC1", "PC2", "PC3", "PC4", "PC5",
        "PC6", "PC7", "PC8", "PC9"), ylab = "Cumulative total inertia (%)")

abline(80,0, col = "blue", lwd = 2, lty = 2)
axis(side=2, at=(seq(0, 100, by=10)), labels = FALSE)

```

# PCA plot construction
```{r, include=FALSE}
# Select only 5 components
nd = 5
Psi = pc1$x[,1:nd]

# STORAGE OF LABELS FOR INDIVIDUALS AND VARIABLES

iden = row.names(dcon)
etiq = names(dcon)
ze = rep(0,length(etiq)) # WE WILL NEED THIS VECTOR AFTERWARDS FOR THE GRAPHICS

# PLOT OF INDIVIDUALS

#Select the component to be plot on our axis
eje1<-1
eje2<-2

# Here is were we create the shadows

plot(Psi[,eje1],Psi[,eje2])
text(Psi[,eje1],Psi[,eje2],labels=iden, cex=0.5)
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

# Now don't display the nodes

plot(Psi[,eje1],Psi[,eje2], type="n")
text(Psi[,eje1],Psi[,eje2],labels=iden, cex=0.5)
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

#Projection of variables
# Correlation between original variables and the principal components
Phi = cor(dcon,Psi)

#select our axis

X<-Phi[,eje1]
Y<-Phi[,eje2]

plot(Psi[,eje1],Psi[,eje2],type="n")
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)

plot(Psi[,eje1],Psi[,eje2], type="n")
#text(Psi[,eje1],Psi[,eje2],labels=iden, cex=0.5)
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")
```

# Projecting the PCA in the Factorial plan.
```{r}
plot(Psi[,eje1],Psi[,eje2],type="n", xlim = c(-1,0.01), ylim = c(-1, 1),
     main = '', xlab = 'PC1', ylab='PC2')
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
# arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.8)
text(-0.65,-0.7, 
     substitute(paste(italic('Note: "price" and "credit" share almost the same space')))
     ,col="red", cex=0.8)

# The "price" and "credit" are overlap, because one is the price of the good that they want to purchase, while the credit is the loan that the client request. And in most of the cases, they are really similar.
# The n_child seems to be opposite than age, which is kind of controversial. The reason of that is, based on the sample size that we are investigating, the younger age tend to have more child than older people.
```

# Zoom to see the aggregated variables among the negative first component
```{r, include=FALSE}
plot(Psi[,eje1],Psi[,eje2],type="n", xlim = c(-1,-0.7), ylim = c(-0.2, 0.2))
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)

# Check that there are not more variables aggregated close to "loan"

plot(Psi[,eje1],Psi[,eje2],type="n", xlim = c(-1,-0.85), ylim = c(-0.1, 0))
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)
```

# To have a better understanding of the factorial planes the target variable "pay" was added into the map. Note that the figure shows that there is a difference on loans that are "payed" or "overdue"

```{r}
# Adding the target variable (index=2)
varcat=factor(data[,2])
plot(Psi[,1],Psi[,2], pch=c(1,20) [varcat], col=c("lightgreen", "red") [varcat],
     xlab = 'PC1', ylab = 'PC2')
axis(side=1, pos= 0, labels = F, col="darkgray")
axis(side=3, pos= 0, labels = F, col="darkgray")
axis(side=2, pos= 0, labels = F, col="darkgray")
axis(side=4, pos= 0, labels = F, col="darkgray")
legend(levels(factor(varcat)),pch=c(1,20),col=c(1,2), cex=0.6)

text(fdic1,fdic2,labels=levels(varcat), col=c("darkblue", "darkblue"), 
     cex=0.7)

```

# If we take a loop in the centroid of distribution of "paid" and "overdue", where's the result.
```{r, include=FALSE}
varcat<-factor(data[,2]) 
fdic1 = tapply(Psi[,eje1],varcat,mean)
fdic2 = tapply(Psi[,eje2],varcat,mean) 

plot(Psi[,eje1],Psi[,eje2],type="n", xlim= c(-1,1), ylim = c(-1,1))
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

text(fdic1,fdic2,labels=levels(varcat), col=c("green", "red"), 
     cex=0.7)
```

# Adding the categorical feature in the factorial plan to find the correlation between variables.
```{r, include=FALSE}
dcat <- c(2:5,11:14,17:18,20)
colors<-rainbow(length(dcat))

plot(Psi[,eje1],Psi[,eje2], type = "n")
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

# Loop to add all categories 

c <- 1
for(k in dcat){
  seguentColor<-colors[c]
  fdic1 = tapply(Psi[,eje1],data[,k],mean)
  fdic2 = tapply(Psi[,eje2],data[,k],mean) 
  
  text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, cex=0.6)
  c<-c+1
}
legend("bottomleft",names(data)[dcat],pch=1,col=colors, cex=0.6)

```

# A zoom is required 
```{r, include=FALSE}
dcat <- c(2:5,11:14,17:18,20)
colors<-rainbow(length(dcat))

plot(Psi[,eje1],Psi[,eje2], type = "n", xlim = c(-3,2), ylim = c(-2,3))
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

# Loop to add all categories 

c <- 1
for(k in dcat){
  seguentColor<-colors[c]
  fdic1 = tapply(Psi[,eje1],data[,k],mean)
  fdic2 = tapply(Psi[,eje2],data[,k],mean) 
  
  text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, cex=0.6)
  c<-c+1
}
legend("bottomleft",names(data)[dcat],pch=1,col=colors, cex=0.6)

```


# As there are too much variables its important to do several maps that will share the same length of the axis to be comparable visually 
# Note: There is a bug in the visualization. The last plot seams smaller but when you save the figure all of them have the same size
```{r}
groups <- list(c(2:5), c(11:13), 14, 17, 18, 20)

for(j in 1:length(groups)){
  colors <- rainbow(length(groups))
  
  a = plot(Psi[,eje1],Psi[,eje2], type = "n", xlim = c(-3,2), ylim = c(-2,3))
  axis(side=1, pos= 0, labels = F, col="cyan")
  axis(side=3, pos= 0, labels = F, col="cyan")
  axis(side=2, pos= 0, labels = F, col="cyan")
  axis(side=4, pos= 0, labels = F, col="cyan")
    
    # Loop to add all categories of this group
    c <- 1
    for(k in groups[[j]]){
      seguentColor<-colors[c]
      fdic1 = tapply(Psi[,eje1],data[,k],mean)
      fdic2 = tapply(Psi[,eje2],data[,k],mean) 
      
      text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, 
           cex=0.6)
      c <- c+1
    }
  legend("bottomleft",names(data)[groups[[j]]],pch=1,col=colors, cex=0.6)
}


```

# Numerical variables and centroids of desired categorical variables
# Several maps are created to see relations
```{r}
groups <- list(c(2:5), c(11:13))

for(j in 1:length(groups)){
  colors <- rainbow(length(groups))
  
  a = plot(Psi[,eje1],Psi[,eje2], type = "n", xlim = c(-1,2), ylim = c(-1,3))
  axis(side=1, pos= 0, labels = F, col="cyan")
  axis(side=3, pos= 0, labels = F, col="cyan")
  axis(side=2, pos= 0, labels = F, col="cyan")
  axis(side=4, pos= 0, labels = F, col="cyan")
  
  #add projections of numerical variables in background
  arrows(ze, ze, X, Y, length = 0.07,col="lightgray")
  text(X,Y,labels=etiq,col="gray", cex=0.7)
    
    # Loop to add all categories of this group
    c <- 1
    for(k in groups[[j]]){
      seguentColor<-colors[c]
      fdic1 = tapply(Psi[,eje1],data[,k],mean)
      fdic2 = tapply(Psi[,eje2],data[,k],mean) 
      
      text(fdic1,fdic2,labels=levels(factor(data[,k])),col=seguentColor, 
           cex=0.6)
      c <- c+1
    }
  legend("bottomleft",names(data)[groups[[j]]],pch=1,col=colors, cex=0.6)
}

```


##### Outlier detection #####

# Once we have the PCA (1rst and 2n component), we can use Mahalanobis distance to detect outlier using with a cutoff of 95%. The orange elipse circle project the 95% of confidence level, while the record outside the surface will be considered as outlier

```{r}
# Plot the PCA plan
plot(Psi[,eje1],Psi[,eje2])
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")

# Defining the parameter for Mahalanobis distance
df <- Psi[,eje1:eje2]
df.center <- colMeans(df)
df.cov <- cov(df)
rad <- sqrt(qchisq(0.95, df = ncol(df)))


# Plotting the ellipse of Mahalanobis distance
ellipse <- data.frame(ellipse(df.center, df.cov, rad, 100, FALSE, col = "orange", fill=TRUE))
```

# Then applying the Mahalanobis distance and the cut-off threshold, we'll find our data treated.
```{r}
# Applying the function
distances <- mahalanobis(x = df, center = df.center, cov = df.cov)

# Defining the Cut-off trigger using Chi-square distance with p = 0.95 df = 2 which in ncol(df)
cutoff <- qchisq(p = 0.95 , df = ncol(df))

# Display all the observations whose distance is greater than the cut-off value.
df[distances > cutoff ,]

# Displan only the observation within the distance
df_clean <- df[distances < cutoff ,]
plot(df_clean, type = "p")
# text(Psi[,eje1],Psi[,eje2],labels=iden,  cex=0.5)
axis(side=1, pos= 0, labels = F, col="cyan")
axis(side=3, pos= 0, labels = F, col="cyan")
axis(side=2, pos= 0, labels = F, col="cyan")
axis(side=4, pos= 0, labels = F, col="cyan")
```


```{r}
#select other axes

eje1 = 3
eje2 = 4

X<-Phi[,eje1]
Y<-Phi[,eje2]

plot(Psi[,eje1],Psi[,eje2],type="n", xlim = c(-.2,1), ylim = c(-1,.2))
axis(side=1, pos= 0, labels = F)
axis(side=3, pos= 0, labels = F)
axis(side=2, pos= 0, labels = F)
axis(side=4, pos= 0, labels = F)
arrows(ze, ze, X, Y, length = 0.07,col="blue")
text(X,Y,labels=etiq,col="darkblue", cex=0.7)
```
It seems that the 4th axis is only related with the id attribute, while the 3rd
might be how often the client accesses the bank. Since they are only
related to variables we already possess, we can conclude that these 
factorial axes aren't relevant in our analysis.
